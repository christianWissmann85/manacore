---
license: mit
task_categories:
  - reinforcement-learning
  - tabular-classification
tags:
  - games
  - magic-the-gathering
  - card-games
  - decision-making
  - mcts
  - strategy
pretty_name: ManaCore MTG Training Data
size_categories:
  - 10K<n<100K
---

# ManaCore: Magic: The Gathering Training Data

High-quality training data for Magic: The Gathering AI research, generated by the ManaCore engine.

## Dataset Description

This dataset contains decision-making data from thousands of MTG games played between AI agents (MCTS, Greedy, Random). Each sample captures:

- **Game state features** (25-dimensional normalized vector)
- **Action taken** (index into legal actions)
- **Legal action count** (for policy head masking)
- **Game outcome** (win/loss/draw from acting player's perspective)

### Intended Use

- Training neural network policies for MTG
- Imitation learning from strong bot play
- Reinforcement learning reward shaping
- Game AI research

## Dataset Structure

### Data Fields

| Field | Type | Description |
|-------|------|-------------|
| `features` | `float32[25]` | Normalized game state features |
| `action` | `int32` | Index of action taken (0 to legal_count-1) |
| `legal_count` | `int32` | Number of legal actions available |
| `action_type` | `string` | Human-readable action type |
| `outcome` | `int32` | Game result: 1 (win), -1 (loss), 0 (draw) |
| `game_id` | `string` | Unique game identifier |
| `turn` | `int32` | Turn number when decision was made |
| `phase` | `string` | Game phase (main1, combat, main2, etc.) |
| `reasoning` | `string` | Optional strategic reasoning (if available) |

### Feature Vector (25 dimensions)

```
[0-1]   Life totals (normalized 0-1, /40)
[2-3]   Creature counts (normalized, /10)
[4-5]   Total power on board (normalized, /30)
[6-7]   Total toughness on board (normalized, /30)
[8-9]   Cards in hand (normalized, /7)
[10-11] Lands in play (normalized, /10)
[12-13] Available mana (normalized, /10)
[14-15] Cards in graveyard (normalized, /20)
[16-17] Cards in library (normalized, /60)
[18]    Current player indicator (1.0 = player, 0.0 = opponent)
[19]    Phase encoding (0.0-1.0)
[20]    Turn number (normalized, /20)
[21]    Life difference (normalized, -1 to 1)
[22]    Creature advantage (normalized, -1 to 1)
[23]    Card advantage (normalized, -1 to 1)
[24]    Mana advantage (normalized, -1 to 1)
```

## Data Formats

### JSONL (Recommended for HuggingFace)

```python
from datasets import load_dataset

# Load from HuggingFace Hub
dataset = load_dataset('Chris-AiKi/manacore-mtg-10k')

# Or load from local JSONL
dataset = load_dataset('json', data_files='training-data.jsonl')
```

### NPZ (NumPy/PyTorch)

```python
import numpy as np

data = np.load('training-data.npz')
features = data['features']      # Shape: (N, 25)
actions = data['actions']        # Shape: (N,)
legal_counts = data['legal_counts']  # Shape: (N,)
outcomes = data['outcomes']      # Shape: (N,)
```

### PyTorch Dataset

```python
import torch
from torch.utils.data import Dataset
import numpy as np

class MTGDataset(Dataset):
    def __init__(self, npz_path):
        data = np.load(npz_path)
        self.features = torch.tensor(data['features'], dtype=torch.float32)
        self.actions = torch.tensor(data['actions'], dtype=torch.long)
        self.outcomes = torch.tensor(data['outcomes'], dtype=torch.float32)
        self.legal_counts = torch.tensor(data['legal_counts'], dtype=torch.long)

    def __len__(self):
        return len(self.actions)

    def __getitem__(self, idx):
        return {
            'features': self.features[idx],
            'action': self.actions[idx],
            'outcome': self.outcomes[idx],
            'legal_count': self.legal_counts[idx],
        }
```

## Generation Details

### Bot Types

| Bot | Description | Strength |
|-----|-------------|----------|
| `random` | Uniform random legal actions | Baseline |
| `greedy` | 1-ply lookahead with heuristics | Medium |
| `mcts` | MCTS with evaluation (200 iterations) | Strong |
| `mcts-strong` | MCTS (500 iterations) | Very Strong |

### Generation Command

```bash
# Generate 10K games of MCTS vs Greedy
bun scripts/generate-batch-data.ts \
  --games 10000 \
  --p1 mcts-eval \
  --p2 greedy \
  --output ./output/mcts-vs-greedy-10k

# Resume interrupted generation
bun scripts/generate-batch-data.ts \
  --resume ./output/mcts-vs-greedy-10k
```

### Data Quality

- All samples from complete, valid games
- Pass-priority actions filtered out (strategic decisions only)
- Outcome labels from actual game results
- Reproducible with seed parameter

## Statistics (Example 10K Dataset)

```
Total Games:     10,000
Total Samples:   ~250,000
Avg Samples/Game: 25
Avg Turns/Game:   12

MCTS Win Rate:   ~65%
Greedy Win Rate: ~30%
Draws:           ~5%
```

## Citation

```bibtex
@software{manacore2024,
  title = {ManaCore: High-Performance MTG Engine for AI Research},
  year = {2024},
  url = {https://github.com/yourusername/manacore}
}
```

## License

MIT License - see [LICENSE](LICENSE) for details.

## Links

- [ManaCore Dataset on HuggingFace](https://huggingface.co/datasets/Chris-AiKi/manacore-mtg-10k)
- [ManaCore Repository](https://github.com/Chris-AiKi/manacore)
- [Training Pipeline Documentation](docs/TRAINING_DATA_PIPELINE.md)
- [Python Gym Environment](packages/python-gym/)
