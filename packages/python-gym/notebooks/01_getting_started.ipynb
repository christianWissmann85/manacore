{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with ManaCore Gym\n",
    "\n",
    "This notebook introduces the ManaCore Gym environment for training AI agents to play Magic: The Gathering.\n",
    "\n",
    "## What you'll learn:\n",
    "1. Basic environment usage\n",
    "2. Understanding observations and actions\n",
    "3. Playing games with random actions\n",
    "4. Training an RL agent with Stable Baselines3\n",
    "\n",
    "## Prerequisites\n",
    "- Run `uv sync --extra notebook` from `packages/python-gym/` directory\n",
    "- Bun runtime installed (for the game server)\n",
    "- **That's it!** The server will auto-start when you run the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Auto-Start Game Server\n",
    "\n",
    "ManaCore Gym needs a game server running on port 3333. This cell will:\n",
    "1. Check if the server is already running\n",
    "2. Start it automatically if needed\n",
    "3. Wait for it to be ready\n",
    "\n",
    "**Note:** The server runs in the background and stays active throughout your session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Environment Usage\n",
    "\n",
    "Let's start by creating an environment and exploring its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def check_server_health():\n",
    "    \"\"\"Check if the ManaCore Gym server is running.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:3333/health\", timeout=1)\n",
    "        return response.status_code == 200\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Start the ManaCore Gym server in the background.\"\"\"\n",
    "    # Find the gym-server directory (relative to this notebook)\n",
    "    notebook_dir = Path.cwd()\n",
    "    gym_server_dir = notebook_dir.parent.parent.parent / \"packages\" / \"gym-server\"\n",
    "    \n",
    "    if not gym_server_dir.exists():\n",
    "        raise FileNotFoundError(f\"Could not find gym-server at: {gym_server_dir}\")\n",
    "    \n",
    "    # Start the server as a background process\n",
    "    server_script = gym_server_dir / \"src\" / \"index.ts\"\n",
    "    process = subprocess.Popen(\n",
    "        [\"bun\", \"run\", str(server_script)],\n",
    "        cwd=str(gym_server_dir),\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "        start_new_session=True  # Detach from notebook process\n",
    "    )\n",
    "    \n",
    "    return process\n",
    "\n",
    "# Check if server is running\n",
    "if check_server_health():\n",
    "    print(\"‚úÖ Game server is already running\")\n",
    "else:\n",
    "    print(\"üöÄ Starting game server...\")\n",
    "    try:\n",
    "        process = start_server()\n",
    "        \n",
    "        # Wait for server to be ready (max 10 seconds)\n",
    "        for _i in range(20):\n",
    "            time.sleep(0.5)\n",
    "            if check_server_health():\n",
    "                print(\"‚úÖ Game server started successfully!\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Server may still be starting. If you get connection errors, wait a few seconds and try again.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to start server: {e}\")\n",
    "        print(\"\\nManual start: Run this in a terminal:\")\n",
    "        print(\"cd packages/gym-server && bun run src/index.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ManaCore Gym version: 0.1.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Import manacore_gym to register the environment\n",
    "import manacore_gym\n",
    "\n",
    "print(\"ManaCore Gym version:\", manacore_gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-1.0, 1.0, (25,), float32)\n",
      "Action space: Discrete(200)\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "# The server will auto-start if not running\n",
    "env = gym.make(\"ManaCore-v0\", opponent=\"greedy\")\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Observations\n",
    "\n",
    "The observation is a 25-dimensional feature vector representing the game state.\n",
    "\n",
    "| Index | Feature | Description |\n",
    "|-------|---------|-------------|\n",
    "| 0-2 | Life totals | Player, opponent, delta |\n",
    "| 3-9 | Board state | Creatures, power, toughness |\n",
    "| 10-14 | Card advantage | Hand size, library size |\n",
    "| 15-18 | Mana | Lands, untapped lands |\n",
    "| 19-24 | Game state | Turn, phase, combat flags |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (25,)\n",
      "Observation dtype: float32\n",
      "\n",
      "Observation values:\n",
      "[1.        1.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.        1.        0.        0.8833333\n",
      " 0.8833333 0.        0.        0.        0.        0.02      1.\n",
      " 0.5       0.        0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment to start a new game\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "print(f\"Observation shape: {obs.shape}\")\n",
    "print(f\"Observation dtype: {obs.dtype}\")\n",
    "print(\"\\nObservation values:\")\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info keys: dict_keys(['turn', 'phase', 'playerLife', 'opponentLife', 'winner', 'stepCount', 'action_mask', 'num_legal_actions', 'legal_actions'])\n",
      "\n",
      "Player life: 20\n",
      "Opponent life: 20\n",
      "Turn: 1\n",
      "Phase: main1\n"
     ]
    }
   ],
   "source": [
    "# The info dict contains additional game information\n",
    "print(\"Info keys:\", info.keys())\n",
    "print(f\"\\nPlayer life: {info.get('playerLife', 'N/A')}\")\n",
    "print(f\"Opponent life: {info.get('opponentLife', 'N/A')}\")\n",
    "print(f\"Turn: {info.get('turn', 'N/A')}\")\n",
    "print(f\"Phase: {info.get('phase', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Actions and Action Masking\n",
    "\n",
    "MTG has variable legal actions depending on game state. We use **action masking** to handle this:\n",
    "- Action space is `Discrete(200)` - max 200 possible actions\n",
    "- `info[\"action_mask\"]` tells you which actions are legal (True = legal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action mask shape: (200,)\n",
      "Action mask dtype: bool\n",
      "\n",
      "Number of legal actions: 4\n",
      "Legal action indices: [0 1 2 3]...\n"
     ]
    }
   ],
   "source": [
    "# Get the action mask\n",
    "action_mask = info[\"action_mask\"]\n",
    "\n",
    "print(f\"Action mask shape: {action_mask.shape}\")\n",
    "print(f\"Action mask dtype: {action_mask.dtype}\")\n",
    "\n",
    "# Find legal actions\n",
    "legal_actions = np.where(action_mask)[0]\n",
    "print(f\"\\nNumber of legal actions: {len(legal_actions)}\")\n",
    "print(f\"Legal action indices: {legal_actions[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_masks() shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "# You can also use the environment's action_masks() method\n",
    "# This is the standard interface for sb3-contrib's MaskablePPO\n",
    "mask = env.unwrapped.action_masks()\n",
    "print(f\"action_masks() shape: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Playing a Game with Random Actions\n",
    "\n",
    "Let's play a complete game using random (but legal) actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Game finished after 19 steps\n",
      "Final reward: -1.0\n",
      "Winner: Opponent\n"
     ]
    }
   ],
   "source": [
    "# Reset for a new game\n",
    "obs, info = env.reset(seed=123)\n",
    "\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "done = False\n",
    "\n",
    "while not done and step_count < 200:\n",
    "    # Get legal actions\n",
    "    mask = info[\"action_mask\"]\n",
    "    legal_actions = np.where(mask)[0]\n",
    "\n",
    "    # Sample a random legal action\n",
    "    action = np.random.choice(legal_actions)\n",
    "\n",
    "    # Take the action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "\n",
    "    # Print progress every 20 steps\n",
    "    if step_count % 20 == 0:\n",
    "        print(f\"Step {step_count}: Life {info.get('playerLife', '?')}/{info.get('opponentLife', '?')}\")\n",
    "\n",
    "# Game result\n",
    "print(f\"\\n{'=' * 40}\")\n",
    "print(f\"Game finished after {step_count} steps\")\n",
    "print(f\"Final reward: {total_reward}\")\n",
    "print(f\"Winner: {'Player' if total_reward > 0 else 'Opponent'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with Stable Baselines3\n",
    "\n",
    "Now let's train an RL agent using MaskablePPO from sb3-contrib.\n",
    "\n",
    "**Note:** Training takes time. For demonstration, we'll use a small number of timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SB3 components\n",
    "try:\n",
    "    from sb3_contrib import MaskablePPO\n",
    "    from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "    SB3_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"sb3-contrib not installed. Run: pip install sb3-contrib\")\n",
    "    SB3_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Model created successfully!\n",
      "Policy architecture: MaskableActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=25, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=25, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=200, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    # Create environment\n",
    "    from manacore_gym import ManaCoreBattleEnv\n",
    "\n",
    "    env = ManaCoreBattleEnv(opponent=\"greedy\")\n",
    "\n",
    "    # Wrap with ActionMasker\n",
    "    def mask_fn(env):\n",
    "        return env.action_masks()\n",
    "\n",
    "    env = ActionMasker(env, mask_fn)\n",
    "\n",
    "    # Create model\n",
    "    model = MaskablePPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=512,  # Smaller for demo\n",
    "        batch_size=64,\n",
    "    )\n",
    "\n",
    "    print(\"Model created successfully!\")\n",
    "    print(f\"Policy architecture: {model.policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34888468d03c46a183d68a423d3ae73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 timesteps (demo)...\n",
      "For real training, use 100k+ timesteps.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 66.3     |\n",
      "|    ep_rew_mean     | -0.714   |\n",
      "| time/              |          |\n",
      "|    fps             | 104      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 512      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 44.3        |\n",
      "|    ep_rew_mean          | -0.739      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 1024        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014504848 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | -0.653      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00477     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.0743      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 45          |\n",
      "|    ep_rew_mean          | -0.758      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 1536        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008444786 |\n",
      "|    clip_fraction        | 0.0281      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | -0.358      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0154     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.0513      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 52.9        |\n",
      "|    ep_rew_mean          | -0.737      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011593467 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.542       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00656    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0409      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.2        |\n",
      "|    ep_rew_mean          | -0.769      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010406626 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0194     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.0368      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.5        |\n",
      "|    ep_rew_mean          | -0.705      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014066281 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00241    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.0465      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.6        |\n",
      "|    ep_rew_mean          | -0.681      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 3584        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010252215 |\n",
      "|    clip_fraction        | 0.0869      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00288    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.0469      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.5        |\n",
      "|    ep_rew_mean          | -0.671      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012526806 |\n",
      "|    clip_fraction        | 0.0873      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 0.0389      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.7        |\n",
      "|    ep_rew_mean          | -0.708      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 4608        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009738848 |\n",
      "|    clip_fraction        | 0.098       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0035     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 0.051       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.1        |\n",
      "|    ep_rew_mean          | -0.68       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008693926 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00878     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 0.0359      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    # Train for a short demo (increase timesteps for real training)\n",
    "    print(\"Training for 5000 timesteps (demo)...\")\n",
    "    print(\"For real training, use 100k+ timesteps.\\n\")\n",
    "\n",
    "    model.learn(total_timesteps=5000, progress_bar=True)\n",
    "\n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating trained agent (10 games)...\n",
      "\n",
      "Game 1: LOSS\n",
      "Game 2: LOSS\n",
      "Game 3: WIN\n",
      "Game 4: LOSS\n",
      "Game 5: LOSS\n",
      "Game 6: LOSS\n",
      "Game 7: LOSS\n",
      "Game 8: LOSS\n",
      "Game 9: LOSS\n",
      "Game 10: LOSS\n",
      "\n",
      "Win rate: 1/10 (10%)\n"
     ]
    }
   ],
   "source": [
    "if SB3_AVAILABLE:\n",
    "    # Evaluate the trained agent\n",
    "    print(\"Evaluating trained agent (10 games)...\\n\")\n",
    "\n",
    "    eval_env = ManaCoreBattleEnv(opponent=\"greedy\")\n",
    "    wins = 0\n",
    "\n",
    "    for game in range(10):\n",
    "        obs, info = eval_env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action_mask = eval_env.action_masks()\n",
    "            action, _ = model.predict(obs, action_masks=action_mask, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "            print(f\"Game {game + 1}: WIN\")\n",
    "        else:\n",
    "            print(f\"Game {game + 1}: LOSS\")\n",
    "\n",
    "    print(f\"\\nWin rate: {wins}/10 ({wins * 10}%)\")\n",
    "    eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "Always close environments when done to release resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment closed.\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "print(\"Environment closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Train longer**: Use `total_timesteps=100_000` or more for meaningful learning\n",
    "2. **Try different opponents**: `random`, `greedy`, `mcts`, `mcts-strong`\n",
    "3. **Vectorized training**: Use `make_vec_env()` for parallel environments\n",
    "4. **Experiment with hyperparameters**: Learning rate, batch size, etc.\n",
    "\n",
    "See the `examples/` directory for more advanced usage:\n",
    "- `train_ppo.py` - Full training script with TensorBoard logging\n",
    "- `evaluate_agent.py` - Comprehensive evaluation against multiple opponents\n",
    "- `benchmark_throughput.py` - Performance benchmarking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manacore-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
